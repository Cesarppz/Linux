{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729e9c5e-95ce-4a8a-a12c-4fc1ffd8280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7f7614-c4d2-477a-90a9-e746f2864598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/felicidad_corrupt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731570df-219e-4f4e-99fc-9056901e386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['country', 'rank', 'score', 'high', 'low'], axis = 1)\n",
    "y = df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4afea9-aac2-4047-b07b-3b96de1162da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709c0e8-f3b3-492d-ab08-90ec0016ddfb",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d46e0a2-310e-46f7-96f6-f61a19a1ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimadores = {\n",
    "    'SVR':SVR(gamma='auto',C = 1.0, epsilon=0.1), # Mediante el parámetro C, podemos controlar la penalización por error en la clasificación. Si C tiene valores amplios entonces, se penaliza de forma más estricta los errores, mientras que si escogemos un C pequeño seremos menos estrictos con los errores.\n",
    "    'RANSAC':RANSACRegressor(),   #RANSAC es un meta estimador, lo que significa que dentro de la función se puede trabajar con diferentes estimadores. El caso predeterminado es con regression lineal RANSACRegressor(LinearRegressor())\n",
    "    'HUBER': HuberRegressor(epsilon=1.35)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae2ce5d4-ed87-4a7d-8720-1053755ba571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVR\n",
      "0.01666975728425138\n",
      "\n",
      "RANSAC\n",
      "1.0227260372817535e-07\n",
      "\n",
      "HUBER\n",
      "1.1830119145007335e-07\n"
     ]
    }
   ],
   "source": [
    "for name , function in estimadores.items():\n",
    "    function.fit(X_train,y_train)\n",
    "    y_predict = function.predict(X_test)\n",
    "    error = mean_squared_error(y_predict,y_test)\n",
    "    print(f'\\n{name}\\n{error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d83bd1-c6d1-4ab6-a0ed-8ae8f66a4451",
   "metadata": {},
   "source": [
    "Como se ve los valores atípicos si influyen en los modelos, por eso los algoritmos que lo tuvieron en cuenta obtuvueron mejores resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ia]",
   "language": "python",
   "name": "conda-env-ia-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
